---
title: "Weight Lifting Movement Prediction"
author: "Jon Cusick"
date: "1/25/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, eval = FALSE)
```

### Intro

### Methods
After downloading the datasets and performing an initial exploratory analysis, a first attempt model was run to see roughly how good some of the basic variables were in prediction. The roll, pitch, and yaw of each of the sensor locations (belt, forearm, arm, and dumbbell) seemed like a good place to start since they explicity captured the movement of each exercise. Pending the results of this set of variables, a second modeling attempt would be to add in the deviation variables of each movement measurement (i.e. stdev, kurtosis, etc.)

Past experience has shown that random forests are some of the best "bang for your buck" models right off the shelf, so I chose a random forest implementation using the `caret` package as a starting point. Though random forests have great predictabiliy prior to a large amount of tuning, a major drawback is that they can run quite slow. I followed the advice from Len Greski's [github site](https://github.com/lgreski/datasciencectacontent/blob/master/markdown/pml-randomForestPerformance.md) and enabled parallel processing of my model.


### Results
In order to validate my model without using any information in the final test set, I split the original training data into a working training/test set, with 75% of the observations going into the training set. I ran the following initial model:
```{r}
rfFit <- train(classe ~ roll_belt + pitch_belt + yaw_belt + roll_forearm + pitch_forearm +
                   yaw_forearm + roll_arm + pitch_arm + yaw_arm + roll_dumbbell + pitch_dumbbell +
                   yaw_dumbbell, method="rf", data=train, trControl=rfFitControl)
```

When training the random forest model, I passed in a command to also run 5 fold cross-validation. In this method, the training set was first split into 5 (roughly) equal subsets. The model was then trained 5 seperate times - in each iteration, a different fold was left out at train time and then used for testing. This allows for a robust estimate for model accuracy, as the accuracy from each of the 5 trained models is then averaged.

![](/Users/JonathanCusick/Documents/local/datasciencecoursera/machine_learning/images/rfFit_confusion_matrix.jpeg){width=70%}

The cross-validation yielded an average overall accuracy of `0.9846`, which lined up closely to the `0.9838` accuracy from running the trained model on the intermediate test set (25% of the original training data).

However, how well would this model be expected to perform on the test set of 20 observations? Namely, what is the expected out of sample error? Simply the cross-validation accuracy subtracted from 1, in this case `0.0154`.